# Kaggle_Titanic
A practice of prediction models, cross validation and parameter tuning with data from kaggle competition 'Titanic'.


# What this work is?
This work is a practice for prediction models. The data was from a Kaggle Competition which was aimming to predict the survial of passagers on Titanic.

# What does it mainly included?
In this work, the core part in this word is the usage of 'Decision Tree', 'Random Forest', 'XGboost' as well as the cross validation and parameter tuning.

# How about the result?
Since it was a pure practice, I did not force myself to get a great mark on the competition. The accuracy for the finial prediction was around 78% which was tested by kaggle. The highest mark in this competition was 100% (I have no idea how did they achieved that).

#
The result will be put on github as CSV files for readers to test, although it was not a very good result. There are three CSV files which are the dataset predicted by the three models (DT, RF and XGboost). Readers can search competition 'Titanic' on Kaggle to reach the website and upload my result to have a test in order to inspire themselves or get familiar with CV, DT, RF, XGboost and etc.

